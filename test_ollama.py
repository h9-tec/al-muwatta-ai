#!/usr/bin/env python3
"""
Test Ollama Integration with Al-Muwatta

This script tests local LLM using Ollama.
"""

import asyncio
from loguru import logger

try:
    from src.services.ollama_service import OllamaService
except ImportError:
    print("âŒ Ollama service not found")
    print("Make sure ollama_service.py exists")
    exit(1)


async def test_ollama():
    """Test Ollama with Arabic and English."""
    
    print("\n" + "="*60)
    print("ğŸ¦™ Testing Ollama Local LLM")
    print("="*60 + "\n")

    try:
        # Initialize service
        print("ğŸ”§ Step 1: Initializing Ollama service...")
        service = OllamaService(model="qwen2.5:7b")
        print("âœ… Service initialized\n")

        # Test 1: English question
        print("ğŸ“ Test 1: English Islamic question")
        print("Question: What are the five pillars of Islam?")
        print("Generating response...")
        
        english_response = await service.generate_content(
            "What are the five pillars of Islam? Be concise.",
            temperature=0.5,
            max_tokens=300,
        )
        
        if english_response:
            print(f"\nâœ… Response:\n{english_response}\n")
        else:
            print("âŒ No response\n")

        # Test 2: Arabic question
        print("-" * 60)
        print("ğŸ“ Test 2: Arabic Islamic question")
        print("Question: Ù…Ø§ Ù‡ÙŠ Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ØŸ")
        print("Generating response...")
        
        arabic_response = await service.generate_content(
            "Ù…Ø§ Ù‡ÙŠ Ø£Ø±ÙƒØ§Ù† Ø§Ù„Ø¥Ø³Ù„Ø§Ù… Ø§Ù„Ø®Ù…Ø³Ø©ØŸ Ø£Ø¬Ø¨ Ø¨Ø´ÙƒÙ„ Ù…Ø®ØªØµØ±.",
            temperature=0.5,
            max_tokens=300,
        )
        
        if arabic_response:
            print(f"\nâœ… Response:\n{arabic_response}\n")
        else:
            print("âŒ No response\n")

        # Test 3: List available models
        print("-" * 60)
        print("ğŸ“Š Available Ollama models:")
        models = service.list_models()
        for model in models:
            print(f"  â€¢ {model}")

        print("\n" + "="*60)
        print("âœ¨ Ollama Testing Complete!")
        print("="*60)
        print("\nğŸ¯ Ollama is working with Al-Muwatta!")
        print("You can now use it instead of Google Gemini\n")

    except Exception as e:
        logger.error(f"Test failed: {e}")
        print(f"\nâŒ Error: {e}")
        print("\nğŸ’¡ Make sure:")
        print("  1. Ollama is installed: curl -fsSL https://ollama.com/install.sh | sh")
        print("  2. Ollama server is running: ollama serve")
        print("  3. Model is downloaded: ollama pull qwen2.5:7b")
        print("  4. Python client installed: pip install ollama\n")


if __name__ == "__main__":
    asyncio.run(test_ollama())

